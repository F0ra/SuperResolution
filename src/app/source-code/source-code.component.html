<p>Python code:</p>
<pre>
  <code>
import os
import PIL
import time
import math
import bcolz
import vgg19
import random
import numpy as np
from PIL import Image
import tensorflow as tf
from functools import wraps
import matplotlib.pyplot as plt

def next_batch(images1,images2,batch_size=4):
    assert images1.shape[0] == images2.shape[0]
    size = images1.shape[0]
    start_index = 0
    end_index = start_index + batch_size
    while end_index <= size:
        yield images1[start_index:end_index], images2[start_index:end_index]
        start_index = end_index
        end_index += batch_size
  
#PIL implimantation
#func returns two images small&big shape=[width,hight,chanels]
#size must be givven in one dim-the biggest one
def image_resize(image,size):
    image_shape = np.array(image)
    
    hight,width,_=image_shape.shape

    scale = max(hight,width)/size
    small_width = math.ceil(width/scale)
    small_hight = math.ceil(hight/scale)
    image_small = np.array(image.resize((small_width,small_hight), Image.BILINEAR))
    image_small_h,image_small_w,image_small_c = image_small.shape
    image_big = np.array(image.resize((4*image_small_w,4*image_small_h), Image.BILINEAR))
    return image_small, image_big

def upsample(image):
    image_shape = np.array(image)
    image = Image.fromarray(image)
    hight,width,_=image_shape.shape

    upsampled_image = np.array(image.resize((4*width,4*hight), Image.BICUBIC))
    return upsampled_image

def lazy_loading(f):
    attribute = '_lazy_' + f.__name__

    @property
    @wraps(f)
    def decor(self):
        if not hasattr(self, attribute):
            with tf.variable_scope(f.__name__):
                setattr(self, attribute, f(self))
        return getattr(self, attribute)

    return decor
      
class superRes:
    
    def __init__(self):
        self.input_image  = tf.placeholder(tf.float32,[None,None,None,3])
        self.target_image = tf.placeholder(tf.float32,[None,None,None,3])
        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False,
                                       name='global_step')      
        self.vgg = vgg19.Vgg19()
    
    @lazy_loading
    def decay_rate(self):
        print("decaying")
        # decay_rate will vanish after 5000 iteration
        return tf.where(self.global_step<5001,500/(500 + tf.cast(self.global_step,tf.float32)), 0 )
    
    #===============building blocks===========================
    def conv(self,bottom,filt_name,filt_shape,use_selu=True):
        
        print("creating variable")        
        filt = tf.Variable(initial_value=tf.truncated_normal(shape = filt_shape,stddev=0.01),name=filt_name)
        bias = tf.Variable(initial_value=tf.truncated_normal(shape = [filt_shape[3],],stddev=0.001),name=filt_name + "bias")
        layer = tf.nn.conv2d(bottom,filt,strides=[1,1,1,1],padding="SAME") + bias
        if use_selu:
            #layer = selu(layer)
            layer = tf.nn.relu(layer)
        return layer
    
    def res_block(self,bottom,name,filt_shape):
        inter_layer = self.conv(bottom,name + "inter",filt_shape)
        return bottom + self.conv(inter_layer,name + "out",filt_shape,use_selu=False)
    
    def upsample(self,bottom,filt_name,filt_shape):
        bottom = tf.keras.layers.UpSampling2D()(bottom)
        return self.conv(bottom,filt_name,filt_shape)
    #===============building blocks===========================
    
    #===super_res_net
    @lazy_loading
    def super_res_out(self):
    
        f = 64
        c = 9
        shape = tf.shape(self.input_image)
        bicubic_out = tf.image.resize_bicubic(self.input_image,[4*shape[1],4*shape[2]])
        self.bicubic_out = tf.image.resize_bicubic(self.input_image,[4*shape[1],4*shape[2]])
        
        conv_9x9_in = self.conv(self.input_image,"conv_9x9_in",[c,c,3,f])

        residual_1 = self.res_block(conv_9x9_in,"residual1",[3,3,f,f])
        residual_2 = self.res_block(residual_1,"residual2",[3,3,f,f])
        residual_3 = self.res_block(residual_2,"residual3",[3,3,f,f])
        residual_4 = self.res_block(residual_3,"residual4",[3,3,f,f])

        upsample_1  = self.upsample(residual_4,"deconv_1",[3,3,f,f])
        upsample_2  = self.upsample(upsample_1,"deconv_2",[3,3,f,f])
        out         = self.conv(upsample_2,"conv_9x9_out",[c,c,f,3],use_selu=False)
        
        #out += self.decay_rate * (self.target_image - 127.5) / 127.5
        out += (bicubic_out - 127.5) / 127.5
        
        self._super_res_out = tf.nn.tanh(out) * 127.5 + 127.5
        return self._super_res_out
        
    @lazy_loading
    def loss(self):
        #self._loss = tf.reduce_mean((self.super_res_out - self.target_image)**2)
        
        loss_0 = tf.reduce_mean((self.super_res_out - self.target_image)**2)
        super_res_out_and_target = tf.concat([self.super_res_out, self.target_image],axis=0)
        self.vgg.build(super_res_out_and_target)
        vgg_conv2_2 = self.vgg.conv2_1
        SRO_conv2_2, target_conv2_2 = tf.split(vgg_conv2_2,num_or_size_splits=2)
        loss_conv2_2 = tf.reduce_mean((target_conv2_2 -  SRO_conv2_2)**2)
        self._loss = 0.8*loss_conv2_2 + 0.2*loss_0
        
        return self._loss
    
    @lazy_loading
    def train(self):
        self._train = tf.train.AdamOptimizer(1e-4).minimize(self.loss, global_step = self.global_step)
        return self._train
      
S=superRes()

path = ('./train2017/')
files = os.walk(path)
for i,z,files_list in files:
    pass
images_num = files_list.__len__()
random.shuffle(files_list)

saver = tf.train.Saver()
with tf.Session() as sess:
    tf.global_variables_initializer().run()  
    ckpt = tf.train.get_checkpoint_state(os.path.dirname('SR_Class/'))
    if ckpt and ckpt.model_checkpoint_path:     
        saver.restore(sess, ckpt.model_checkpoint_path)
        print('model restore from checkpoints')
        
        
    for k in range(images_num):
        image = Image.open(path+files_list[k])
        try:
            np.array(image).shape[2]==3
        except:
            print("Skiped!")
            continue
        #im_size = np.random.randint(70,150)
        im_size = 150
        input_img, big_img = image_resize(image,im_size)
        input_img, big_img = input_img[np.newaxis,...], big_img[np.newaxis,...]
        feed_dict=[S.input_image:input_img ,S.target_image:big_img]
        
        <!-- feed_dict={S.input_image:input_img ,S.target_image:big_img} -->
        #sess.run(S.train,feed_dict=feed_dict)
        if k % 10==1:
            saver.save(sess, 'SR_Class/sr.ckpt')
               
  </code>
</pre>
